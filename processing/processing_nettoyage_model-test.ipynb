{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md1",
   "metadata": {},
   "source": [
    "# Processing, Nettoyage & Modélisation\n",
    "\n",
    "Variable cible : **`log_prevalence`** (taux normalisé par région = `Ntop / Npop`)"
   ]
  },
  {
   "cell_type": "code",
   "id": "c1",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as ss\n",
    "\n",
    "df = pd.read_csv('effectifs.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md2",
   "metadata": {},
   "source": [
    "## 1. Nettoyage des données\n",
    "\n",
    "On supprime :\n",
    "- Les lignes où `Ntop` est manquant (nécessaire pour calculer la prévalence)\n",
    "- La région 99 (France entière) qui biaiserait l'analyse régionale"
   ]
  },
  {
   "cell_type": "code",
   "id": "c2",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Suppression des lignes où Ntop est manquant\n",
    "df = df.dropna(subset=['Ntop'])\n",
    "\n",
    "# Suppression de la région 99 (France entière)\n",
    "df.drop(df[df['region'] == 99].index, inplace=True)\n",
    "\n",
    "print('Shape après nettoyage :', df.shape)\n",
    "print('Valeurs manquantes sur Ntop :', df['Ntop'].isnull().sum())\n",
    "print('Régions présentes :', sorted(df['region'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md3",
   "metadata": {},
   "source": [
    "## 2. Agrégation et recalcul de la prévalence\n",
    "\n",
    "On agrège par `(annee, region, patho_niv1)` et on recalcule :\n",
    "\n",
    "> `prevalence = ΣNtop / ΣNpop`\n",
    "\n",
    "La colonne `prev` originale n'est **pas utilisée** car :\n",
    "- Calculée à un niveau trop fin (département + âge + sexe)\n",
    "- Non comparable entre régions\n",
    "- Contient trop de NaN"
   ]
  },
  {
   "cell_type": "code",
   "id": "c3",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_model = (\n",
    "    df.groupby(['annee', 'region', 'patho_niv1'])\n",
    "    .agg({'Ntop': 'sum', 'Npop': 'sum'})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "df_model['prevalence'] = df_model['Ntop'] / df_model['Npop']\n",
    "\n",
    "print('Shape df_model :', df_model.shape)\n",
    "print('NaN dans prevalence :', df_model['prevalence'].isnull().sum())\n",
    "df_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md4",
   "metadata": {},
   "source": [
    "## 3. Transformation logarithmique de la variable cible\n",
    "\n",
    "La prévalence est asymétrique (skewness ≈ 2.77 d'après l'EDA).\n",
    "\n",
    "On applique `log1p(prevalence)` pour :\n",
    "- Corriger l'asymétrie\n",
    "- Réduire l'impact des outliers\n",
    "- Stabiliser les prédictions\n",
    "\n",
    "`log1p(x) = log(1 + x)` fonctionne même sur les valeurs proches de 0"
   ]
  },
  {
   "cell_type": "code",
   "id": "c4",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_model['log_prevalence'] = np.log1p(df_model['prevalence'])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].hist(df_model['prevalence'], bins=50, color='steelblue')\n",
    "axes[0].set_title('Distribution prevalence (originale)')\n",
    "\n",
    "axes[1].hist(df_model['log_prevalence'], bins=50, color='green')\n",
    "axes[1].set_title('Distribution log_prevalence (après transformation)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Skewness avant : {df_model['prevalence'].skew():.2f}\")\n",
    "print(f\"Skewness après : {df_model['log_prevalence'].skew():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md5",
   "metadata": {},
   "source": [
    "## 4. Définition des features et split train/test\n",
    "\n",
    "**Features retenues :** `annee`, `patho_niv1`, `region`\n\n",
    "**Variable cible :** `log_prevalence`\n\n",
    "**Variables exclues :**\n",
    "- `prev` originale : trop de NaN, niveau trop fin\n",
    "- `Npop` : data leakage potentiel\n",
    "- `Ntop` : utilisé pour calculer la cible\n\n",
    "**Split 80/20** avec `random_state=42`"
   ]
  },
  {
   "cell_type": "code",
   "id": "c5",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = ['annee', 'patho_niv1', 'region']\n",
    "target   = 'log_prevalence'\n",
    "\n",
    "X = df_model[features]\n",
    "y = df_model[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print('X_train :', X_train.shape)\n",
    "print('X_test  :', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md6",
   "metadata": {},
   "source": [
    "## 5. Préprocesseur & Pipeline\n",
    "\n",
    "- `annee`, `region` → pipeline **numérique** (imputer médiane + StandardScaler)\n",
    "- `patho_niv1` → pipeline **catégoriel** (imputer mode + OrdinalEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "id": "c6",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numeric_features     = ['annee', 'region']\n",
    "categorical_features = ['patho_niv1']\n",
    "\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler',  StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OrdinalEncoder())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_pipeline,     numeric_features),\n",
    "    ('cat', categorical_pipeline, categorical_features)\n",
    "])\n",
    "\n",
    "print(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md7",
   "metadata": {},
   "source": [
    "## 6. Modèle 1 — Régression Linéaire"
   ]
  },
  {
   "cell_type": "code",
   "id": "c7",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "lin_pipeline = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "lin_pipeline.fit(X_train, y_train)\n",
    "y_pred_lin = lin_pipeline.predict(X_test)\n",
    "\n",
    "rmse_lin = np.sqrt(mean_squared_error(y_test, y_pred_lin))\n",
    "r2_lin   = r2_score(y_test, y_pred_lin)\n",
    "\n",
    "print('Linear Regression')\n",
    "print('RMSE:', rmse_lin)\n",
    "print('R2  :', r2_lin)\n",
    "\n",
    "joblib.dump(lin_pipeline, '../model/linear.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md8",
   "metadata": {},
   "source": [
    "## 7. Modèle 2 — Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "id": "c8",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('model', RandomForestRegressor(n_estimators=300, random_state=42))\n",
    "])\n",
    "\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "y_pred_rf = rf_pipeline.predict(X_test)\n",
    "\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "r2_rf   = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print('Random Forest')\n",
    "print('RMSE:', rmse_rf)\n",
    "print('R2  :', r2_rf)\n",
    "\n",
    "joblib.dump(rf_pipeline, '../model/random_forest.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md9",
   "metadata": {},
   "source": [
    "## 8. Modèle 3 — Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "id": "c9",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gb_pipeline = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('model', GradientBoostingRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "gb_pipeline.fit(X_train, y_train)\n",
    "y_pred_gb = gb_pipeline.predict(X_test)\n",
    "\n",
    "rmse_gb = np.sqrt(mean_squared_error(y_test, y_pred_gb))\n",
    "r2_gb   = r2_score(y_test, y_pred_gb)\n",
    "\n",
    "print('Gradient Boosting')\n",
    "print('RMSE:', rmse_gb)\n",
    "print('R2  :', r2_gb)\n",
    "\n",
    "joblib.dump(gb_pipeline, '../model/gradient_boosting.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md10",
   "metadata": {},
   "source": [
    "## 9. Comparaison des 3 modèles"
   ]
  },
  {
   "cell_type": "code",
   "id": "c10",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\n",
    "    'Modèle' : ['Linear Regression', 'Random Forest', 'Gradient Boosting'],\n",
    "    'RMSE'   : [rmse_lin, rmse_rf, rmse_gb],\n",
    "    'R2'     : [r2_lin,   r2_rf,   r2_gb]\n",
    "})\n",
    "\n",
    "print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md11",
   "metadata": {},
   "source": [
    "## 10. Test de data leakage\n",
    "\n",
    "On mélange la target d'entraînement et on vérifie que le R2 chute proche de 0.\n",
    "\n",
    "Si le R2 reste élevé → data leakage détecté."
   ]
  },
  {
   "cell_type": "code",
   "id": "c11",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_train_shuffled = np.random.permutation(y_train)\n",
    "\n",
    "rf_pipeline.fit(X_train, y_train_shuffled)\n",
    "y_pred_shuffled = rf_pipeline.predict(X_test)\n",
    "\n",
    "r2_leak_test = r2_score(y_test, y_pred_shuffled)\n",
    "print('R2 avec target mélangée :', r2_leak_test)\n",
    "print('-> Pas de data leakage' if r2_leak_test < 0.1 else '⚠️ Data leakage potentiel détecté')"
   ]
  }
 ]
}